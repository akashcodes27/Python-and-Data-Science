'''
ACME insurance is an insurance company that offers affordable health insurance to thousands of customers. Lets assume that I am the lead data scientist at ACME insurance. I have been tasked with building an automated system(basically a trained ML-model end-to-end pipeline) which is able to estimate the medical expenditure for the company's new customers. In order to make such an estimation, I am supposed to take following features into consideration such as: age, sex, BMI, smoking habits, children, region of residence.
Estimate that our system generates for each customer will enable us to determine how much insurance to charge from every customer
'''




from urllib.request import urlretrieve

medical_data_url = 'https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv'
urlretrieve(medical_data_url, 'medical_insurance.csv')

//We used urlretrieve function from the urllib.request module. This function we have used for retrieving csv insurance data. In our case, we have the csv dataset hosted on github. BUT heres a critical mistake that many could make. We cannot directly use the github link for retrieveing data. We need to click on a button that says 'RAW". This opens the link that has this csv file hosted. This is the link that we will be using


Remember all these are pandas functions, as are doing exploratory data analysis and manipulation using the pandas library 


successfully imported the dataset
medical_df = pd.read_csv('medical_insurance.csv')
medical_df.head(3)

# So the "charges" column is our output variable(or the variable that we need to predict using a regression model), and all other attributes are going to be used as input variables. We basically use hostorical data to train our model and to find patterns betwee input attributes and the output attribute "charges". We can then use this system to predict charges for new customers.

medical_df.info()   pandas function
the info() gives me the following information: attribute names, their datatypes, null or not-null constraint, total number of rows
Lets understand how the Dtypes look like in scikit learn:



medical_df.describe()
the describe() gives me some basic statistical information about the dataset
describe() is extremely useful as it gives us some information like: mean, std deviation, min, max, 50%, 70%, 25%
By analyzing these metrics we can make a bunch of inferences like: For eg: Lets consider attribute "age"
By observing medical_df, we can infer that maximum age of customer is 64 and minimum age of customer is 18 implying people outside this age range cannot register for the insurance 
If we observe the static metric 75%, it gives us information that holds true for 75% of the data.
"age": 51  (75% of customers are aged 51)
"bmi": 34 (75% of customers have class 1 obesity)
"children": 2 (75% of customers have 2 children)
"charges": 16639 (75% of customers spend 16639$ on their medical bills)
Just like 75%, we can also refer to "mean", 50%, 25%, min, max to draw different inferences 



Now alongside Exploratory Analysis, lets also do some visualization. For the purpose of visualization, we will use sonme libraries like: Matplotlib, Seaborn, Plotly

import matplotlib.pyplot as plt
import seaborn as sns 
import plotly.express as px   #plotly is used to make the chart interactive
%matplotlib inline   #this makes sure charts are rederred in code and not in external popup

We will provide some settings to improve default style and font sizes 
sns.set_style('darkgrid')
matplotlib.rcParams[font.size] = 14 
matplotlib.rcParams[figure.figsize] = (10, 6)
matplotlib.rcParams[figure.facecolor] = #00000000


fig = px.histogram(
    medical_df,
    x = 'age',
    nbins = 47,
    title = 'Age Distribution Histogram'
)

fig.update_layout(bargap=0.1)
fig.show()


fig = px.histogram(
    medical_df,
    x = 'age',
    color = 'sex',
    color_discrete_sequence = ['maroon', 'blue],
    #here we have the option of providing only a single color which is the default usual behavior, but we can provide 2 colors to then see the difference in distribution for male/female, smoker/smoker, depends on yes/no attribute that we have provided in color attribute 
    nbins = 47,
    title = 'Age Distribution Histogram'
)

fig.update_layout(bargap=0.2)
fig,show()



#Lets look at example 2
fig1 = px.histogram(
    medical_df,
    x = 'charges',
    nbins = 100,
    marginal = 'box'   #this also adds a box plot for both smokers and non smokers 
    color = 'smoker',
    color_discrete_sequence = ['red','blue'],
    title = 'charges distribution between smoker and non-smokers'
)
fig1.update_layout(bargap=0.3)
fig1.show()

# What can we infer from the histogram visualization that we have created below. 
# Most non-smokers have less charges then smokers
# Majority of customers are non-smokers and the remaining are smokers 
# However these inferences are just assumptions. From this data we can infer that, smokers pay higher insurance premiums than non-smokers because of associated health risks. 
# This graph follows the power law or we can also call it exponential.
# What does that mean? What it means is that we can see higher numbers for the initial groups of customers and these numbers start to gradually, exponentially decline as we move to the later groups of customers 
# And according to the power law, usually these initial groups of customers that seem to have the lowest medical charges are represent or constitute the majority. 


fig2 = px.histogram(
    medical_df,
    x = 'charges',
    nbins = 100,
    color = 'region',
    color_discrete_sequence=['blue', 'green', 'purple', 'orange'],
    marginal = 'box',
    title = 'charges distribution between regions'
)

fig2.update_layout(bargap = 0.3)
fig2.show()


medical_df.region.value_counts()
medical_df.sex.value_counts()
medical_df.smoker.value_counts()


px.histogram(
    medical_df, 
    x = 'smoker',
    color = 'sex',
    color_discrete_sequence = ['blue', 'green'],
    title = 'Smoker Distribution'
)


Remember: Whenever we are performing data analysis, exploration, visualization, if we are playing around with sample, it is necessary to ensure that the distributions of the samples match the distribution of the population, as the sample is the representative of the population.



# Lets visualize the relationship between Age and Sum of charges for smokers and non-smokers of different age groups
fig3 = px.histogram(
    medical_df, 
    x= 'age',
    y = 'charges',
    color = 'smoker',
    title = 'Relationship between age and charges'
)

fig3.update_layout(bargap = 0.3)
fig3.show()



# So we can observe that majority of non-smokers have lower insurance charges. Except a small proportion of outliers who may have higher insurance charges
# due to genetic reasons or chronic illnesses
# Whereas smokers on an average have higher insurance charges, and ofcourse even smokers have a dedicated proportion of outliers who may have higher
# insurance charges due to chronic illnesses and genetic reasons



So far, lets quickly summarize what we looked at: 
We have X Attributes: A1, A2, A3, A4, A5, A6, A7, A8 (age, sex, BMI, smoker, gender, Children etc)
We have Y Attribute: C (Charge)
 

So far, we plotted: Histogram, Scatter Plot to determine relationships between the following: 
A1 and c 
A2 and c 
A3 and c 
A4 and c 
We carefully observed the plots to find if any meaningful visual relationships exist between these pairs of traits 
Althought histogram and scatterplots dont fully work for all kinds, so we also use some others like: violin plot(to help us observe the density of these relationships when we deal with discrete variables(and not gradients))
Like usually in scatterplots with gradient attributes, we can clearly observe density by looking at clusters of scatterpoints
But when dealing with a pair such as: Children and Charge 
We wont be able to view density of these plots(like eg: if we have 1000 people having 2 children, then does the majority of these 1000 have high medical charge or low medical charge)

Lets now understand co-relation
In simple terms Correlation helps us make relative comparisons between different pairs of attributes to figure out which ones have a stronger relationship. 

Eg: Correlation between BMI and charge is: 0.19
EG: Correlation between Age and charge is: 0.39 
We compute this using the (.corr()) function 
This relative comparison allows to come to a conclusion that relationship between BMI and charge is relatively useless as compared to relationship between Age and Charge 

lets see the code:
medical_df.charges.corr(medical_df.age)
medical_df.charges.corr(medical_df.bmi)
medical_df.charges.corr(medical_df.children)

But so far we only looked at the numeric attributes, what if we are dealing with categorical attributes 

medical_df.charges.corr(medical_df.smoker.map({'no': 0, 'yes': 1}))
After executing this line in google colab, we could clearly observe that there is a very strong Correlation between Charges attribute and Categorical attribute Smoker 

medical_df.charges.corr(medical_df.sex.map({'male':1, 'female':0}))

medical_df.charges.corr(medical_df.children)

Correlation Coeficients can be interpreted in 2 ways:
1. Strength 
2. Direction 

Strength: The absolute value of the correlation coefficients. Extreme values +1 and -1 indicate strong +ve or -ve correlation. Closer the value to any of these values +1 or -1, stronger the correlation. 0 indicates, no correlation at all. 

1. A perfectly linear relationship line: value almost +1
2. A strong linear relationship line: value  closer to 1 but not high enough 
+ve or -ve value also decides the direction of the relationship. 
+Ve indicates: When one variable increase, other increases as well 
-ve indicates: When one increases, other decreases 
0 indicates points are all over the place with no trend 




Lets do some crazy stuff.
Lets convert all the YES/NO, MALE/FEMALE cartegorical attributes into 1 and 0
medical_df['smoker'] = medical_df.smoker.map({'yes':1, 'no':0})
medical_df['sex'] = medical_df.sex.map({'male':1, 'female':0})
medical_df.drop('region', axis=1, inplace=True)
medical_df.corr()

medical_df.corr() prints an entire table of correlations of every attribute with every attribute 
To better understand and visualize these attributes, we use heatmaps 

sns.heatmap(medical_df, cmap='Reds', annot=True)
plt.title('Correlation matrix')



I wasnt fully able to grasp the difference between: Correlation and Causation. Try to fully understand the difference between the two. Conclusion: Correlation isn't necessarily related to causation

LINEAR REGRESSION:
Linear Regression is a fundamental supervised machine learning algorithm that is used to model the relationship between dependent target variable and multiple independent variables(or features) by finding a straight line through the data. It works by finding the Best-Fit Line represented by the equation y = mx + c. This line captures the linear trend in the data showing us how the change in the independent variable causes or influences a change in the dependent variable.
'charges' attribute is related to 'age' attribute by y=mx + c relation. 

So whenever we use a Linear Regression model with any new data, the model tries to analyze and make sense of data to find the best fit line for the data. Once it fully understands how output changes with respect to input, we can then provide the trained model with inputs that it has not previously seen and model can accurately make a prediction based on the historic trend that it identified    


We shall now compare the correlation between 'charges' and 'age' for a non-smoker 
The dataframe that we have contains both: smoker and non-smokers, lets filter it to only have smoker.



non_smoker_df = medical_df[medical_df['smoker'] == 0]


We shall now visualize the relationship between both the attributes:

We can plot scatterplots through 2 ways. (1st: Seaborn, 2nd: Plotly)
px.scatter(medical_df, x='age', y='charges')
sns.scatterplot(data=medical_df, x='age', y='charges')
We observe a linear trend between the two attributes 

So: in the video that we are watching. We can clearly see that we are making use of the "straight line equation" y=mx+c to model correlation between charges and age. 
The person has manually defined a function that performs y=mx+c and returns the value of 'y'(as it represents estimated charges)

However we have manually defined the values for m and c (slope and intercept)
Hence we plot the calculated values to see what kind of fit we have with the predicted line. If we are not satisfied, we can simply alter the values for slope(m ) and intercept(c) to make it align with observed scatterplot trend. 


In this case, we are not using a machine learning model. we are instead modeling this linear relationship manually by manualy defining linear equation and slope as well as y-intercept values 


def linear_func(slope, intercept, x):
  return slope * x + intercept 

slope = 200
intercept = 100


non_smoker_df = medical_df[medical_df['smoker'] == 0]
calculated_charges = linear_func(slope, intercept, non_smoker_df.age)
ages = non_smoker_df.age

plt.plot(ages, calculated_charges)
plt.scatter(ages, non_smoker_df.charges)



Loss/Cost function:

We can compare our model's predictions with the actual targets using the following method: 

RMSE(Root Mean Squared Error)
We take difference of ACTUAL VALUE and CALCULATED VALUE.
We square this difference 
We take Average of this result 
We take square root of the whole thing 
This results in formation of a very powerful evaluation metric which we call RMSE(Root Mean Squared Error)


Lets understand Root Mean Squared Error
RMSE tells how much on average the real values differ from the calculated values. 

targets = non_smoker_df['charges']
calculated_charges = linear_func(slope, intercept, non_smoker_df.age)

def rmse(targets, calculated_charges):
    np.sqrt(np.mean(np.square(targets - calculated_charges)))

rmse(targets, calculated_charges)
we got a value of approx 4500. This value tells us that the target deviates from calculated_charges by approx Loss Function value of 4500
To fix this, we would need to play around with the values of Slope and Intercept


We Call Slope and Intercept as WEIGHTS. We need a strategy to optimize the weights such that we have a better fit with the actual observations .(To Reduce LOSS and to improve the FIT of the data )

So we have 2 ways to make that happen: Ordinary Least Squares, Stochastic Gradient Descent 
These 2 methods are used to optimize the weights(Intercept and Slope) to find the Best Fit Line for the correlation between the 2 attributes . What is the role of these optimizers: To minimize the loss by trying to find the best fit line 



Ordinary Least Squares: Uses combination of Calculation and Linear Algebra to find the best fit weights w and b (does matrice conversions and inversions) [Works best for smaller datasets]

Stochastic Gradient Descent: very surface level info: Imagine you are on a hill in dark, you want to go down, you try taking one step down in every direction and once you find the step to take, you continue taking more steps down and this cycle repeats.
[works best for larger datasets]

My Opinion: I feel like the Linear Regression machine learning models from scikit learn already come with all these features embedded. Right now we manually wrote an algorithm to find the best fit line for finding correlation between 2 attributes. And we are manually calculating weights and also using a manual approach for optimizing weights to minimize the error margin or loss functio value, to accomplish that we are using techniques like Ordinary Least Squares, Stochastic Gradient Descent. 
But I feel like many built in models already have these optimizers embedded within the algorithm , and possibly all we do is train and tune hyperparameters of the model using tools offered to us by ML libraries 



The below I did not understand , how the first one is a series and second one a dataframe?
non_smoker_df.age is a series 
non_smoker_df[['age']] is a dataframe

So basically even though age is just one column, we still treat it like as if it is a 2-dimensional array with one column 
When we import the LinearRegression model from sklearn.linear_model, to fit the model to the data, we need inputs as dataframe not as series 


Here we will feed the model with 2 attributes: 'age' and 'charges' 

inputs = non_smoker_df[['age']]
targets = non_smoker_df.charges 

model.fit(inputs, targets)

When we import the LinearRegression model from sklearn.linear_model, we are already telling the model to FIND linear or directly proportional relationships between the specified attributes 
So when we use fit function to train the model, what we so far did manually the model will now do that internally to the data which we have passed to it. Since the model is trained for the purpose of making predictions on the data that it has not previously seen, it needs to internally make use of optimizers and reduce loss function to find a best fit linear line of relationship that passes closely along the data which we have passed

print(inputs.shape)
print(targets.shape)
print(inputs.size)
print(targets.size)

# Size tells us only the number of rows
# shape tells us the number of rows as well as number of columns